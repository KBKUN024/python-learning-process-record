{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "text = \"\"\"\n",
    "dno   dname   dloc       \n",
    "10    会计部   北京\n",
    "20    研发部   成都\n",
    "30    销售部   重庆\n",
    "40    运维部   深圳\n",
    "\"\"\"\n",
    "text2 = \"\"\"\n",
    "eno     ename    job        mgr      sal     comm    dno\n",
    "1359    胡一刀    销售员      3344.0    1800    200.0    30\n",
    "2056    乔峰      分析师      7800.0    5000    1500.0   20\n",
    "3088    李莫愁    设计师      2056.0    3500    800.0    20\n",
    "3211    张无忌     程序员     2056.0    3200    NaN     20\n",
    "3233    丘处机     程序员     2056.0    3400    NaN     20\n",
    "3244    欧阳锋     程序员     3088.0    3200    NaN     20\n",
    "3251    张翠山     程序员     2056.0    4000    NaN     20\n",
    "3344    黄蓉      销售主管    7800.0    3000    800.0   30\n",
    "3577    杨过      会计       5566.0    2200     NaN     10\n",
    "3588    朱九真     会计       5566.0    2500    NaN     10\n",
    "4466    苗人凤     销售员     3344.0    2500    NaN     30\n",
    "5234    郭靖       出纳      5566.0    2000    NaN      10\n",
    "5566    宋远桥     会计师     7800.0    4000    1000.0   10\n",
    "7800    张三丰     总裁       NaN      9000    1200.0    20\n",
    "\"\"\"\n",
    "text3 = \"\"\"\n",
    "eno    ename    job      mgr      sal    comm    dno                              \n",
    "9500   张三丰   总裁      NaN      50000  8000    20\n",
    "9600   王大锤   程序员    9800.0   8000   600     20\n",
    "9700   张三丰   总裁      NaN      60000  6000    20\n",
    "9800   骆昊     架构师    7800.0   30000  5000    20\n",
    "9900   陈小刀   分析师    9800.0   10000  1200    20\n",
    "\"\"\"\n",
    "\n",
    "# 使用read_csv配合StringIO来正确解析数据\n",
    "# 对于空格分隔的数据，使用raw string或者re.compile来避免转义警告\n",
    "df = pd.read_csv(StringIO(text.strip()), sep=r\"\\s+\")\n",
    "df2 = pd.read_csv(StringIO(text2.strip()), sep=r\"\\s+\")\n",
    "df3 = pd.read_csv(StringIO(text3.strip()), sep=r\"\\s+\")\n",
    "\n",
    "# 设置dno为index\n",
    "df.set_index(\"dno\", inplace=True)\n",
    "\n",
    "# 保存文件时处理空值问题\n",
    "df.to_csv(\"dept.csv\")\n",
    "df2.to_csv(\"emp_df.csv\", index=False, na_rep=\"NaN\")  # 空值用空字符串表示，避免连续逗号\n",
    "df3.to_csv(\"emp2_df.csv\", index=False, na_rep=\"NaN\")  # 空值用空字符串表示，避免连续逗号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ename   job     mgr   sal    comm  dno\n",
      "eno                                        \n",
      "1359   胡一刀   销售员  3344.0  1800   200.0   30\n",
      "2056    乔峰   分析师  7800.0  5000  1500.0   20\n",
      "3088   李莫愁   设计师  2056.0  3500   800.0   20\n",
      "3211   张无忌   程序员  2056.0  3200     NaN   20\n",
      "3233   丘处机   程序员  2056.0  3400     NaN   20\n",
      "3244   欧阳锋   程序员  3088.0  3200     NaN   20\n",
      "3251   张翠山   程序员  2056.0  4000     NaN   20\n",
      "3344    黄蓉  销售主管  7800.0  3000   800.0   30\n",
      "3577    杨过    会计  5566.0  2200     NaN   10\n",
      "3588   朱九真    会计  5566.0  2500     NaN   10\n",
      "4466   苗人凤   销售员  3344.0  2500     NaN   30\n",
      "5234    郭靖    出纳  5566.0  2000     NaN   10\n",
      "5566   宋远桥   会计师  7800.0  4000  1000.0   10\n",
      "7800   张三丰    总裁     NaN  9000  1200.0   20\n",
      "==================================================\n",
      "     ename  job     mgr    sal  comm  dno\n",
      "eno                                      \n",
      "9500   张三丰   总裁     NaN  50000  8000   20\n",
      "9600   王大锤  程序员  9800.0   8000   600   20\n",
      "9700   张三丰   总裁     NaN  60000  6000   20\n",
      "9800    骆昊  架构师  7800.0  30000  5000   20\n",
      "9900   陈小刀  分析师  9800.0  10000  1200   20\n",
      "==================================================\n",
      "    dname dloc\n",
      "dno           \n",
      "10    会计部   北京\n",
      "20    研发部   成都\n",
      "30    销售部   重庆\n",
      "40    运维部   深圳\n"
     ]
    }
   ],
   "source": [
    "emp = pd.read_csv(\"./emp_df.csv\", index_col=\"eno\")\n",
    "emp2 = pd.read_csv(\"./emp2_df.csv\", index_col=\"eno\")\n",
    "dept = pd.read_csv(\"./dept.csv\", index_col=\"dno\")\n",
    "print(emp)\n",
    "print(\"=\" * 50)\n",
    "print(emp2)\n",
    "print(\"=\" * 50)\n",
    "print(dept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ename   job     mgr    sal    comm  dno\n",
      "eno                                         \n",
      "1359   胡一刀   销售员  3344.0   1800   200.0   30\n",
      "2056    乔峰   分析师  7800.0   5000  1500.0   20\n",
      "3088   李莫愁   设计师  2056.0   3500   800.0   20\n",
      "3211   张无忌   程序员  2056.0   3200     NaN   20\n",
      "3233   丘处机   程序员  2056.0   3400     NaN   20\n",
      "3244   欧阳锋   程序员  3088.0   3200     NaN   20\n",
      "3251   张翠山   程序员  2056.0   4000     NaN   20\n",
      "3344    黄蓉  销售主管  7800.0   3000   800.0   30\n",
      "3577    杨过    会计  5566.0   2200     NaN   10\n",
      "3588   朱九真    会计  5566.0   2500     NaN   10\n",
      "4466   苗人凤   销售员  3344.0   2500     NaN   30\n",
      "5234    郭靖    出纳  5566.0   2000     NaN   10\n",
      "5566   宋远桥   会计师  7800.0   4000  1000.0   10\n",
      "7800   张三丰    总裁     NaN   9000  1200.0   20\n",
      "9500   张三丰    总裁     NaN  50000  8000.0   20\n",
      "9600   王大锤   程序员  9800.0   8000   600.0   20\n",
      "9700   张三丰    总裁     NaN  60000  6000.0   20\n",
      "9800    骆昊   架构师  7800.0  30000  5000.0   20\n",
      "9900   陈小刀   分析师  9800.0  10000  1200.0   20\n"
     ]
    }
   ],
   "source": [
    "all_emp_df = pd.concat([emp, emp2])\n",
    "print(all_emp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_emp_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 这里在重设什么索引？就是重新让all_emp_df的索引变为默认的从0开始是不是？\n",
    "是的，完全正确！\n",
    "\n",
    "`reset_index(inplace=True)` 的作用是：\n",
    "- 重置索引为默认的数字索引：将当前的索引（这里是 eno 员工编号）变回从 0 开始的默认数字索引\n",
    "- 原索引变成普通列：原来的 eno 索引会变成一个名为 eno 的普通数据列\n",
    "- 就地修改：inplace=True 表示直接修改 all_emp_df 本身，而不是返回新的 DataFrame\n",
    "\n",
    "为什么要这样做？\n",
    "\n",
    "从上下文看，后面的 pd.merge() 使用了 on=\"dno\" 参数来连接，这要求 dno 是普通列而不是索引。重置索引后，eno 变成普通列，dno 列就可以正常用于连接操作了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     eno ename   job     mgr    sal    comm  dno\n",
      "0   1359   胡一刀   销售员  3344.0   1800   200.0   30\n",
      "1   2056    乔峰   分析师  7800.0   5000  1500.0   20\n",
      "2   3088   李莫愁   设计师  2056.0   3500   800.0   20\n",
      "3   3211   张无忌   程序员  2056.0   3200     NaN   20\n",
      "4   3233   丘处机   程序员  2056.0   3400     NaN   20\n",
      "5   3244   欧阳锋   程序员  3088.0   3200     NaN   20\n",
      "6   3251   张翠山   程序员  2056.0   4000     NaN   20\n",
      "7   3344    黄蓉  销售主管  7800.0   3000   800.0   30\n",
      "8   3577    杨过    会计  5566.0   2200     NaN   10\n",
      "9   3588   朱九真    会计  5566.0   2500     NaN   10\n",
      "10  4466   苗人凤   销售员  3344.0   2500     NaN   30\n",
      "11  5234    郭靖    出纳  5566.0   2000     NaN   10\n",
      "12  5566   宋远桥   会计师  7800.0   4000  1000.0   10\n",
      "13  7800   张三丰    总裁     NaN   9000  1200.0   20\n",
      "14  9500   张三丰    总裁     NaN  50000  8000.0   20\n",
      "15  9600   王大锤   程序员  9800.0   8000   600.0   20\n",
      "16  9700   张三丰    总裁     NaN  60000  6000.0   20\n",
      "17  9800    骆昊   架构师  7800.0  30000  5000.0   20\n",
      "18  9900   陈小刀   分析师  9800.0  10000  1200.0   20\n",
      "==================================================\n",
      "    dname dloc\n",
      "dno           \n",
      "10    会计部   北京\n",
      "20    研发部   成都\n",
      "30    销售部   重庆\n",
      "40    运维部   深圳\n",
      "     eno ename  job     mgr   sal  comm  dno dname dloc\n",
      "10  4466   苗人凤  销售员  3344.0  2500   NaN   30   会计部   北京\n"
     ]
    }
   ],
   "source": [
    "print(all_emp_df)\n",
    "print(\"=\" * 50)\n",
    "print(dept)\n",
    "new_df = pd.merge(all_emp_df, dept, left_index=True, right_index=True)\n",
    "print(new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的`new_df`输出的是：\n",
    "\n",
    "```\n",
    "     eno ename  job     mgr   sal  comm  dno dname dloc\n",
    "10  4466   苗人凤  销售员  3344.0  2500   NaN   30   会计部   北京\n",
    "```\n",
    "\n",
    "是因为：\n",
    "\n",
    "首先`left_index=True`，也就是`all_emp_df`使用`索引`进行连接，`right_index=True`所以`dept`也是使用`索引`进行连接，那么就去比较左表和右表中索引相同的项，左右表中都能找到索引为10的项，右表的索引是10-40，而左表的索引只到18为止，所以只有一条记录，如果左表有一个索引为20，那么会有两条记录。假如左表中存在一个索引为50的记录，那么这条记录不会出现在最终的new_df中，因为右表中没有。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 请为我说明 how 属性的各个值的区别，以及怎么用\n",
    "\n",
    "**how 参数的值及其区别**\n",
    "\n",
    "`pd.merge()` 中的 how 参数决定了连接的方式：\n",
    "\n",
    "四种连接类型\n",
    "\n",
    "- inner（内连接）：只保留两个 DataFrame 中都存在的键值\n",
    "  - 结果：交集，最少记录\n",
    "- outer（外连接）：保留两个 DataFrame 中所有的键值\n",
    "  - 结果：并集，最多记录，缺失值用 NaN 填充\n",
    "- left（左连接）：保留左边 DataFrame 的所有记录\n",
    "  - 结果：以左表为主，右表匹配不上的用 NaN 填充\n",
    "- right（右连接）：保留右边 DataFrame 的所有记录\n",
    "  - 结果：以右表为主，左表匹配不上的用 NaN 填充\n",
    "\n",
    "我的代码中:`new_df = pd.merge(all_emp_df, dept, how=\"inner\", on=\"dno\")`\n",
    "\n",
    "- 使用 inner 连接，只会保留 all_emp_df 和 dept 中都存在相同 dno 值的记录\n",
    "- 如果某个员工的 dno 在部门表中不存在，该员工记录会被过滤掉\n",
    "- 如果某个部门没有员工，该部门记录也不会出现在结果中\n",
    "\n",
    "选择建议：根据业务需求决定，通常用 left 保留所有员工信息，用 inner 只要有效的员工-部门对应关系。\n",
    "\n",
    "### 2. `on` 参数的用法\n",
    "\n",
    "`on` 参数指定用于连接的列名（键）：\n",
    "\n",
    "#### 基本用法\n",
    "\n",
    "- `on=\"列名\"`：当两个 DataFrame 有相同列名时\n",
    "  - `pd.merge(df1, df2, on=\"dno\")` # 用 dno 列连接\n",
    "- `on=[\"列 1\", \"列 2\"]`：多列连接\n",
    "  - `pd.merge(df1, df2, on=[\"dno\", \"job\"])` # 同时匹配部门和职位\n",
    "\n",
    "### 3. 请为我说明，merge 的 left_index 和 right_index 还有 left_on 还有 right_on 的用法是什么？\n",
    "\n",
    "`left_index` 和 `right_index`\n",
    "**含义**\n",
    "\n",
    "指定是否使用索引作为连接键：\n",
    "- `left_index=True`：左表使用索引而不是某一列做连接\n",
    "- `right_index=True`：右表使用索引而不是某一列做连接\n",
    "\n",
    "为什么是 True/False：\n",
    "- True：用索引连接\n",
    "- False（默认）：用列连接\n",
    "\n",
    "索引是什么索引？\n",
    "- 索引就是 DataFrame 的行标签\n",
    "- 默认索引：0, 1, 2, 3... (数字序号)\n",
    "- 如果read_csv的时候，设置了index_col='dno'这样的属性的话，那么此时dno作为当前DataFrame的索引\n",
    "\n",
    "用途: 指定是否使用 DataFrame 的索引进行连接\n",
    "\n",
    "```python\n",
    "# 当其中一个表的连接键是索引时\n",
    "pd.merge(df1, df2, left_on=\"dno\", right_index=True)\n",
    "# 或者两个表都用索引连接\n",
    "pd.merge(df1, df2, left_index=True, right_index=True)\n",
    "```\n",
    "适用场景: 当你的连接键设置为了 DataFrame 的索引时使用\n",
    "\n",
    "---\n",
    "\n",
    "`left_on` 和 `right_on`\n",
    "\n",
    "用途: 当两个 DataFrame 中用于连接的列名不同时使用\n",
    "\n",
    "```python\n",
    "# 左表用 dno 列做连接键，右表用 dept_id 列做连接键\n",
    "pd.merge(emp_df, dept_df, left_on=\"dno\", right_on=\"dept_id\")\n",
    "```\n",
    "---\n",
    "\n",
    "vs `on` 参数的区别:\n",
    "\n",
    "- `on=\"列名\"`: 两个表的连接列名相同时使用\n",
    "- `left_on + right_on`: 两个表的连接列名不同时使用\n",
    "---\n",
    "\n",
    "实际应用示例\n",
    "\n",
    "```python\n",
    "# 你的代码等价于：\n",
    "new_df = pd.merge(all_emp_df, dept, left_on=\"dno\", right_on=\"dno\")\n",
    "# 因为两表的连接列名都是 \"dno\"，所以可以简化为：\n",
    "new_df = pd.merge(all_emp_df, dept, on=\"dno\")\n",
    "```\n",
    "选择建议: 列名相同用 on，列名不同用 left_on/right_on，索引连接用 left_index/right_index。\n",
    "#### 记忆要点：`index参数控制\"用不用索引\"，on参数控制\"用哪一列\"`。\n",
    "### 4. 左右的概念（left,right）\n",
    "```\n",
    "pd.merge(df1,df2,)\n",
    "          👇  👇\n",
    "         左表 右表\n",
    "```\n",
    "### 5. merge的连接方式\n",
    "默认是使用inner连接的，假如你使用了:`new_df = pd.merge(all_emp_df, dept, left_index=True, right_index=True)`,发现没有显式指定how的值，那么默认就是inner连接。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据清洗\n",
    "通常，我们从 Excel、CSV 或数据库中获取到的数据并不是非常完美的，里面可能因为系统或人为的原因混入了重复值或异常值，也可能在某些字段上存在缺失值；再者，`DataFrame`中的数据也可能存在格式不统一、量纲不统一等各种问题。因此，在开始数据分析之前，对数据进行清洗就显得特别重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ename    job    mgr    sal   comm    dno\n",
      "eno                                           \n",
      "1359  False  False  False  False  False  False\n",
      "2056  False  False  False  False  False  False\n",
      "3088  False  False  False  False  False  False\n",
      "3211  False  False  False  False   True  False\n",
      "3233  False  False  False  False   True  False\n",
      "3244  False  False  False  False   True  False\n",
      "3251  False  False  False  False   True  False\n",
      "3344  False  False  False  False  False  False\n",
      "3577  False  False  False  False   True  False\n",
      "3588  False  False  False  False   True  False\n",
      "4466  False  False  False  False   True  False\n",
      "5234  False  False  False  False   True  False\n",
      "5566  False  False  False  False  False  False\n",
      "7800  False  False   True  False  False  False\n",
      "==================================================\n",
      "      ename    job    mgr    sal   comm    dno\n",
      "eno                                           \n",
      "1359  False  False  False  False  False  False\n",
      "2056  False  False  False  False  False  False\n",
      "3088  False  False  False  False  False  False\n",
      "3211  False  False  False  False   True  False\n",
      "3233  False  False  False  False   True  False\n",
      "3244  False  False  False  False   True  False\n",
      "3251  False  False  False  False   True  False\n",
      "3344  False  False  False  False  False  False\n",
      "3577  False  False  False  False   True  False\n",
      "3588  False  False  False  False   True  False\n",
      "4466  False  False  False  False   True  False\n",
      "5234  False  False  False  False   True  False\n",
      "5566  False  False  False  False  False  False\n",
      "7800  False  False   True  False  False  False\n"
     ]
    }
   ],
   "source": [
    "print(emp.isnull())\n",
    "print(\"=\" * 50)\n",
    "print(emp.isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ename   job     mgr   sal    comm  dno\n",
      "eno                                        \n",
      "1359   胡一刀   销售员  3344.0  1800   200.0   30\n",
      "2056    乔峰   分析师  7800.0  5000  1500.0   20\n",
      "3088   李莫愁   设计师  2056.0  3500   800.0   20\n",
      "3344    黄蓉  销售主管  7800.0  3000   800.0   30\n",
      "5566   宋远桥   会计师  7800.0  4000  1000.0   10\n"
     ]
    }
   ],
   "source": [
    "print(emp.dropna())  # 删除所有那些出现过NaN的行，也就是说，一行中的所有值都不能是NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ename   job   sal  dno\n",
      "eno                        \n",
      "1359   胡一刀   销售员  1800   30\n",
      "2056    乔峰   分析师  5000   20\n",
      "3088   李莫愁   设计师  3500   20\n",
      "3211   张无忌   程序员  3200   20\n",
      "3233   丘处机   程序员  3400   20\n",
      "3244   欧阳锋   程序员  3200   20\n",
      "3251   张翠山   程序员  4000   20\n",
      "3344    黄蓉  销售主管  3000   30\n",
      "3577    杨过    会计  2200   10\n",
      "3588   朱九真    会计  2500   10\n",
      "4466   苗人凤   销售员  2500   30\n",
      "5234    郭靖    出纳  2000   10\n",
      "5566   宋远桥   会计师  4000   10\n",
      "7800   张三丰    总裁  9000   20\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    emp.dropna(axis=1)\n",
    ")  # 删除列中包含有NaN的列，可以从上面的输出看出来，mgr和comm这两列中存在有NaN，所以这两列都被删除了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ename   job     mgr   sal    comm  dno\n",
      "eno                                        \n",
      "1359   胡一刀   销售员  3344.0  1800   200.0   30\n",
      "2056    乔峰   分析师  7800.0  5000  1500.0   20\n",
      "3088   李莫愁   设计师  2056.0  3500   800.0   20\n",
      "3211   张无忌   程序员  2056.0  3200     0.0   20\n",
      "3233   丘处机   程序员  2056.0  3400     0.0   20\n",
      "3244   欧阳锋   程序员  3088.0  3200     0.0   20\n",
      "3251   张翠山   程序员  2056.0  4000     0.0   20\n",
      "3344    黄蓉  销售主管  7800.0  3000   800.0   30\n",
      "3577    杨过    会计  5566.0  2200     0.0   10\n",
      "3588   朱九真    会计  5566.0  2500     0.0   10\n",
      "4466   苗人凤   销售员  3344.0  2500     0.0   30\n",
      "5234    郭靖    出纳  5566.0  2000     0.0   10\n",
      "5566   宋远桥   会计师  7800.0  4000  1000.0   10\n",
      "7800   张三丰    总裁     0.0  9000  1200.0   20\n"
     ]
    }
   ],
   "source": [
    "print(emp.fillna(0))  # 或者 emp.fillna(value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重复数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    dname dloc\n",
      "dno           \n",
      "10    会计部   北京\n",
      "20    研发部   成都\n",
      "30    销售部   重庆\n",
      "40    运维部   深圳\n",
      "50    研发部   上海\n",
      "60    销售部   长沙\n"
     ]
    }
   ],
   "source": [
    "dept.loc[50] = {\"dname\": \"研发部\", \"dloc\": \"上海\"}\n",
    "dept.loc[60] = {\"dname\": \"销售部\", \"dloc\": \"长沙\"}\n",
    "print(dept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 这里为什么要用loc？如果是整数索引的话，不应该使用iloc吗？\n",
    "这里使用 `loc` 是正确的，因为：\n",
    "\n",
    "`loc` 是基于`标签（label）`的索引，这里的 50 和 60 是作为索引标签来使用的，而不是位置。\n",
    "`iloc` 是基于`位置（position）`的索引，如果使用 `iloc[50]` 意味着访问*第51个位置的行*（从0开始计数）。\n",
    "在这个例子中：\n",
    "- `dept.loc[50]` 创建或访问索引标签为 50 的行\n",
    "- `dept.loc[60]` 创建或访问索引标签为 60 的行\n",
    "如果改用 `iloc[50]`，pandas 会尝试访问第51个位置的行，这很可能会报错（因为DataFrame可能没有那么多行）。\n",
    "\n",
    "#### 记忆要点：`loc = 标签索引`，`iloc = 位置索引`。即使标签看起来像整数，仍然要用 loc。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    dname dloc\n",
      "dno           \n",
      "10    会计部   北京\n",
      "20    研发部   成都\n",
      "30    销售部   重庆\n",
      "40    运维部   深圳\n"
     ]
    }
   ],
   "source": [
    "print(dept.drop_duplicates(\"dname\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    dname dloc\n",
      "dno           \n",
      "10    会计部   北京\n",
      "40    运维部   深圳\n",
      "50    研发部   上海\n",
      "60    销售部   长沙\n"
     ]
    }
   ],
   "source": [
    "print(dept.drop_duplicates(\"dname\", keep=\"last\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keep 参数用于指定当发现重复数据时，保留哪一个：\n",
    "\n",
    "- keep=`\"first\"`（默认）：保留第一次出现的数据，删除后面的重复项\n",
    "- keep=`\"last\"`：保留最后一次出现的数据，删除前面的重复项\n",
    "- keep=`False`：删除所有重复项，一个都不保留\n",
    "\n",
    "在你的例子中，keep=\"last\" 意味着如果 dname 列有重复值，会保留最后出现的那一行，删除之前的重复行。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "不使用drop:\n",
      "==================================================\n",
      "    index   eno ename   job     mgr    sal    comm  dno\n",
      "0       0  1359   胡一刀   销售员  3344.0   1800   200.0   30\n",
      "1       1  2056    乔峰   分析师  7800.0   5000  1500.0   20\n",
      "2       2  3088   李莫愁   设计师  2056.0   3500   800.0   20\n",
      "3       3  3211   张无忌   程序员  2056.0   3200     NaN   20\n",
      "4       4  3233   丘处机   程序员  2056.0   3400     NaN   20\n",
      "5       5  3244   欧阳锋   程序员  3088.0   3200     NaN   20\n",
      "6       6  3251   张翠山   程序员  2056.0   4000     NaN   20\n",
      "7       7  3344    黄蓉  销售主管  7800.0   3000   800.0   30\n",
      "8       8  3577    杨过    会计  5566.0   2200     NaN   10\n",
      "9       9  3588   朱九真    会计  5566.0   2500     NaN   10\n",
      "10     10  4466   苗人凤   销售员  3344.0   2500     NaN   30\n",
      "11     11  5234    郭靖    出纳  5566.0   2000     NaN   10\n",
      "12     12  5566   宋远桥   会计师  7800.0   4000  1000.0   10\n",
      "13     13  7800   张三丰    总裁     NaN   9000  1200.0   20\n",
      "14     15  9600   王大锤   程序员  9800.0   8000   600.0   20\n",
      "15     17  9800    骆昊   架构师  7800.0  30000  5000.0   20\n",
      "16     18  9900   陈小刀   分析师  9800.0  10000  1200.0   20\n",
      "==================================================\n",
      "使用drop:\n",
      "==================================================\n",
      "     eno ename   job     mgr    sal    comm  dno\n",
      "0   1359   胡一刀   销售员  3344.0   1800   200.0   30\n",
      "1   2056    乔峰   分析师  7800.0   5000  1500.0   20\n",
      "2   3088   李莫愁   设计师  2056.0   3500   800.0   20\n",
      "3   3211   张无忌   程序员  2056.0   3200     NaN   20\n",
      "4   3233   丘处机   程序员  2056.0   3400     NaN   20\n",
      "5   3244   欧阳锋   程序员  3088.0   3200     NaN   20\n",
      "6   3251   张翠山   程序员  2056.0   4000     NaN   20\n",
      "7   3344    黄蓉  销售主管  7800.0   3000   800.0   30\n",
      "8   3577    杨过    会计  5566.0   2200     NaN   10\n",
      "9   3588   朱九真    会计  5566.0   2500     NaN   10\n",
      "10  4466   苗人凤   销售员  3344.0   2500     NaN   30\n",
      "11  5234    郭靖    出纳  5566.0   2000     NaN   10\n",
      "12  5566   宋远桥   会计师  7800.0   4000  1000.0   10\n",
      "13  7800   张三丰    总裁     NaN   9000  1200.0   20\n",
      "14  9600   王大锤   程序员  9800.0   8000   600.0   20\n",
      "15  9800    骆昊   架构师  7800.0  30000  5000.0   20\n",
      "16  9900   陈小刀   分析师  9800.0  10000  1200.0   20\n"
     ]
    }
   ],
   "source": [
    "new_df1 = all_emp_df.drop_duplicates([\"ename\", \"job\"])\n",
    "new_df2 = all_emp_df.drop_duplicates([\"ename\", \"job\"])\n",
    "new_df1.reset_index(inplace=True)  # 不使用drop\n",
    "print(f\"{'='*50}\\n不使用drop:\\n{'='*50}\\n{new_df1}\")\n",
    "new_df2.reset_index(drop=True, inplace=True)  # 使用drop\n",
    "print(f\"{'='*50}\\n使用drop:\\n{'='*50}\\n{new_df2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop参数有什么用？\n",
    "作用: 决定是否保留原来的索引：\n",
    "- `drop=True`：丢弃原索引，只生成新的连续索引（0,1,2...）\n",
    "- `drop=False`（默认）：把原索引保存为新的一列，同时生成新索引\n",
    "\n",
    "简单说：`drop=True 避免把旧索引变成新列，保持DataFrame列数不变`。\n",
    "\n",
    "看上面也知道，因为使用了drop_duplicates，删去了ename和job一样的行（张三丰的那几行，且只保留了一个张三丰），原来的索引就会出现缺失，此时索引不再连续，我想让它连续，就要用reset_index，重新设置索引，并用drop丢弃掉原来的索引，这样新得到的DataFrame的索引就连续了。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 异常值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "异常值在统计学上的全称是疑似异常值，也称作离群点（outlier），异常值的分析也称作离群点分析。异常值是指样本中出现的“极端值”，数据值看起来异常大或异常小，其分布明显偏离其余的观测值。实际工作中，有些异常值可能是由系统或人为原因造成的，但有些异常值却不是，它们能够重复且稳定的出现，属于正常的极端值，例如很多游戏产品中头部玩家的数据往往都是离群的极端值。所以，我们既不能忽视异常值的存在，也不能简单地把异常值从数据分析中剔除。重视异常值的出现，分析其产生的原因，常常成为发现问题进而改进决策的契机。\n",
    "\n",
    "异常值的检测有`Z-score` 方法、`IQR` 方法、`DBScan 聚类`、`孤立森林`等，这里我们对前两种方法做一个简单的介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3和Q1怎么算？\n",
    "**Q1和Q3的计算步骤**：\n",
    "1. 排序：将数据从小到大排列\n",
    "2. 找位置：\n",
    "    - `Q1位置 = (n+1) × 0.25`\n",
    "    - `Q3位置 = (n+1) × 0.75`\n",
    "    - n是`数据个数`\n",
    "3. 取值：\n",
    "    - 如果位置是整数，直接取该位置的值\n",
    "    - 如果位置是小数，在相邻两个值之间线性插值\n",
    "例子： 数据 [1,2,3,4,5,6,7,8,9]\n",
    "- n=9，Q1位置=(9+1)×0.25=2.5 → Q1=2.5\n",
    "- Q3位置=(9+1)×0.75=7.5 → Q3=7.5\n",
    "\n",
    "pandas中直接用： `df.quantile([0.25, 0.75])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `IQR = Q3 - Q1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1945 entries, 0 to 1944\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype         \n",
      "---  ------  --------------  -----         \n",
      " 0   销售日期    1945 non-null   datetime64[ns]\n",
      " 1   销售区域    1945 non-null   object        \n",
      " 2   销售渠道    1945 non-null   object        \n",
      " 3   品牌      1945 non-null   object        \n",
      " 4   售价      1945 non-null   int64         \n",
      "dtypes: datetime64[ns](1), int64(1), object(3)\n",
      "memory usage: 76.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "sales_df = pd.read_excel(\n",
    "    \"./2020年销售数据.xlsx\",\n",
    "    usecols=[\"销售日期\", \"销售区域\", \"销售渠道\", \"品牌\", \"售价\"],\n",
    ")\n",
    "print(sales_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           销售日期 销售区域 销售渠道    品牌   售价  月份  季度  星期\n",
      "0    2020-01-01   上海  拼多多   八匹马   99   1   1   2\n",
      "1    2020-01-01   上海   抖音   八匹马  219   1   1   2\n",
      "2    2020-01-01   上海   天猫   八匹马  169   1   1   2\n",
      "3    2020-01-01   上海   天猫   八匹马  169   1   1   2\n",
      "4    2020-01-01   上海   天猫   皮皮虾  249   1   1   2\n",
      "...         ...  ...  ...   ...  ...  ..  ..  ..\n",
      "1940 2020-12-30   北京   京东  花花姑娘  269  12   4   2\n",
      "1941 2020-12-30   福建   实体   八匹马   79  12   4   2\n",
      "1942 2020-12-31   福建   实体  花花姑娘  269  12   4   3\n",
      "1943 2020-12-31   福建   抖音   八匹马   59  12   4   3\n",
      "1944 2020-12-31   福建   天猫   八匹马   99  12   4   3\n",
      "\n",
      "[1945 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "sales_df[\"月份\"] = sales_df[\"销售日期\"].dt.month\n",
    "sales_df[\"季度\"] = sales_df[\"销售日期\"].dt.quarter\n",
    "sales_df[\"星期\"] = sales_df[\"销售日期\"].dt.weekday\n",
    "print(sales_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3140 entries, 0 to 3139\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   city             3140 non-null   object\n",
      " 1   companyFullName  3140 non-null   object\n",
      " 2   positionName     3140 non-null   object\n",
      " 3   salary           3140 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 98.3+ KB\n",
      "None\n",
      "==================================================\n",
      "  city companyFullName positionName   salary\n",
      "0   北京  达疆网络科技（上海）有限公司        数据分析岗  15k-30k\n",
      "1   北京    北京音娱时光科技有限公司         数据分析  10k-18k\n",
      "2   北京   北京千喜鹤餐饮管理有限公司         数据分析  20k-30k\n",
      "3   北京   吉林省海生电子商务有限公司         数据分析  33k-50k\n",
      "4   北京  韦博网讯科技（北京）有限公司         数据分析  10k-15k\n"
     ]
    }
   ],
   "source": [
    "jobs_df = pd.read_csv(\n",
    "    \"./某招聘网站招聘数据.csv\",\n",
    "    usecols=[\"city\", \"companyFullName\", \"positionName\", \"salary\"],\n",
    ")\n",
    "print(jobs_df.info())\n",
    "print(\"=\" * 50)\n",
    "print(jobs_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的数据表一共有`3140`条数据，但并非所有的职位都是“数据分析”的岗位，如果要筛选出数据分析的岗位，可以通过检查`positionName`字段是否包含“数据分析”这个关键词，这里需要模糊匹配，应该如何实现呢？我们可以先获取`positionName`列，因为这个`Series`对象的`dtype`是字符串，所以可以通过`str`属性获取对应的字符串向量，然后就可以利用我们熟悉的字符串的方法来对其进行操作，代码如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1515, 4)\n"
     ]
    }
   ],
   "source": [
    "jobs_df = jobs_df[jobs_df.positionName.str.contains(\"数据分析\")]\n",
    "print(jobs_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0   1\n",
      "0     15  30\n",
      "1     10  18\n",
      "2     20  30\n",
      "3     33  50\n",
      "4     10  15\n",
      "...   ..  ..\n",
      "3065   8  10\n",
      "3069   6  10\n",
      "3070   2   4\n",
      "3071   6  12\n",
      "3088   8  12\n",
      "\n",
      "[1515 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(jobs_df.salary.str.extract(r\"(\\d+)[kK]?-(\\d+)[kK]?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 为什么这里要jobs_df.salary.str，为什么要点一下str？\n",
    "因为 `jobs_df.salary` 是一个 pandas Series对象，不是字符串类型。\n",
    "\n",
    "`.str` 属性是 pandas 专门为处理字符串数据提供的字符串访问器，它可以：\n",
    "- 将 Series 中的每个元素当作字符串来操作\n",
    "- 提供字符串方法的向量化版本（如 .extract()、.contains()、.replace() 等）\n",
    "- 对整列数据批量执行字符串操作\n",
    "\n",
    "如果直接用 `jobs_df.salary.extract()`，Python 会报错，因为 Series 对象本身没有 `extract()` 方法。只有通过 `.str 访问器`，才能使用字符串的正则表达式方法来提取薪资范围中的数字。\n",
    "\n",
    "### 2. 向量化版本？和向量有什么关系？向量是什么库的概念，numpy吗？还是pandas？\n",
    "向量化是 numpy 的核心概念，pandas 继承了这个思想。\n",
    "\n",
    "向量化指的是：\n",
    "- 一次操作应用于整个数组/Series，而不是逐个元素循环\n",
    "- 底层用 C 实现，比 Python 循环快几十倍\n",
    "- 避免显式的 for 循环\n",
    "例如：\n",
    "```python\n",
    "# 非向量化（慢）\n",
    "for i in range(len(series)):\n",
    "    result[i] = series[i].extract(pattern)\n",
    "\n",
    "# 向量化（快）\n",
    "series.str.extract(pattern)  # 一次性处理整列\n",
    "```\n",
    "所以 .str 提供的是向量化的字符串方法 - 一个函数调用处理整列数据，而不是逐行处理。这就是为什么 pandas 比纯 Python 循环快得多的原因。\n",
    "### 3. numpy和pandas的区别\n",
    "numpy：\n",
    "- 专注数值计算，处理同质数据（相同类型）\n",
    "- 核心是多维数组（ndarray）\n",
    "- 提供数学函数、线性代数、随机数等\n",
    "- 更底层，性能更高\n",
    "\n",
    "pandas：\n",
    "- 专注数据分析，处理异质数据（不同类型）\n",
    "- 核心是DataFrame和Series（带标签的数据结构）\n",
    "- 提供数据清洗、分组、合并、时间序列等\n",
    "- 基于numpy构建，更高层次\n",
    "### 4. Series和字典的区别和关系\n",
    "关系：Series 本质上是\"增强版字典\"\n",
    "\n",
    "相同点：\n",
    "- 都是键值对结构\n",
    "- 都可以通过键（索引）访问值\n",
    "\n",
    "关键区别：\n",
    "\n",
    "字典：\n",
    "```python\n",
    "data = {'a': 10, 'b': 20, 'c': 30}\n",
    "data['a']  # 10\n",
    "```\n",
    "Series:\n",
    "```python\n",
    "s = pd.Series({'a': 10, 'b': 20, 'c': 30})\n",
    "s['a']     # 10\n",
    "s + 5      # 所有值都加5，字典做不到\n",
    "s.mean()   # 计算平均值，字典做不到\n",
    "```\n",
    "核心差异：\n",
    "- 字典只能单个操作\n",
    "- Series可以批量操作（向量化）+ 数据分析功能\n",
    "\n",
    "所以Series = 字典的数据结构 + numpy的计算能力 + pandas的分析功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  city companyFullName positionName  salary\n",
      "0   北京  达疆网络科技（上海）有限公司        数据分析岗    22.5\n",
      "1   北京    北京音娱时光科技有限公司         数据分析    14.0\n",
      "2   北京   北京千喜鹤餐饮管理有限公司         数据分析    25.0\n",
      "3   北京   吉林省海生电子商务有限公司         数据分析    41.5\n",
      "4   北京  韦博网讯科技（北京）有限公司         数据分析    12.5\n"
     ]
    }
   ],
   "source": [
    "temp_df = jobs_df.salary.str.extract(r\"(\\d+)[kK]?-(\\d+)[kK]?\").map(int)\n",
    "jobs_df[\"salary\"] = temp_df.apply(np.mean, axis=1)\n",
    "print(jobs_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. applymap和apply的作用和用法详解\n",
    "\n",
    "#### applymap() - 元素级操作\n",
    "- 作用：对DataFrame的**每个元素**应用函数\n",
    "- 适用于：需要对每个单独的值进行转换的场景\n",
    "- 注意：**已弃用**，建议使用 `.map()` 替代\n",
    "\n",
    "在上面的代码中：\n",
    "```python\n",
    "temp_df = jobs_df.salary.str.extract(r\"(\\d+)[kK]?-(\\d+)[kK]?\").applymap(int)\n",
    "```\n",
    "这里 `applymap(int)` 将提取出的每个字符串数字转换为整数类型\n",
    "等价的新写法：`temp_df.map(int)`\n",
    "\n",
    "#### apply() - 行/列级操作  \n",
    "- 作用：对DataFrame的**行或列**应用函数\n",
    "- axis=0：对每列操作（默认）\n",
    "- axis=1：对每行操作\n",
    "\n",
    "在上面的代码中：\n",
    "```python\n",
    "jobs_df[\"salary\"] = temp_df.apply(np.mean, axis=1)\n",
    "```\n",
    "这里 `apply(np.mean, axis=1)` 计算每行的平均值\n",
    "- axis=1 表示沿着行方向（即对每行的列进行操作）\n",
    "- 结果：将\"15-25\"这样的区间转换为平均值20\n",
    "\n",
    "#### 核心区别总结：\n",
    "- **applymap/map**：元素 → 元素（单个值变换）\n",
    "- **apply**：行/列 → 值（聚合或变换整行/列）\n",
    "\n",
    "示例对比：\n",
    "```python\n",
    "df = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n",
    "\n",
    "# applymap/map：每个元素+1\n",
    "df.map(lambda x: x + 1)     # [[2,4], [3,5]]\n",
    "\n",
    "# apply：每行求和\n",
    "df.apply(sum, axis=1)       # [4, 6]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始DataFrame:\n",
      "   A  B\n",
      "0  1  3\n",
      "1  2  4\n",
      "\n",
      "每个元素加1后:\n",
      "   A  B\n",
      "0  2  4\n",
      "1  3  5\n",
      "\n",
      "转换为字典格式验证:\n",
      "{'A': [2, 3], 'B': [4, 5]}\n",
      "\n",
      "每行求和:\n",
      "0    4\n",
      "1    6\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]})\n",
    "print(\"原始DataFrame:\")\n",
    "print(df)\n",
    "print(\"\\n每个元素加1后:\")\n",
    "result = df.map(lambda x: x + 1)\n",
    "print(result)\n",
    "print(\"\\n转换为字典格式验证:\")\n",
    "print(result.to_dict(\"list\"))\n",
    "print(\"\\n每行求和:\")\n",
    "print(df.apply(sum, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "      A  B     意思是：    {\"A\": [2, 3], \"B\": [4, 5]}\n",
    "   0  2  4     →  第0行：A=2, B=4\n",
    "   1  3  5         第1行：A=3, B=5\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-learning-process-record",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
